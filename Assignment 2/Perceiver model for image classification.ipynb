{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Perceiver model for image classification.ipynb","provenance":[],"authorship_tag":"ABX9TyNZwqWFbHwZsSMlBPJJVjvh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_o-n6nJXGyt6"},"source":["Assignment - 2\n","Description: Implementing the Perceiver model for image classification."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCxWNhT4pLGx","executionInfo":{"status":"ok","timestamp":1633734487088,"user_tz":420,"elapsed":4705,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}},"outputId":"4b86fc1a-b205-473c-bee0-f4e52d83e505"},"source":["pip install -U tensorflow-addons"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 962 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 993 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.14.0\n"]}]},{"cell_type":"code","metadata":{"id":"Ko0h22EcG7aF","executionInfo":{"status":"ok","timestamp":1633734489622,"user_tz":420,"elapsed":2537,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UBpuvrQHIhk"},"source":["Prepare the data & configure the hyperparameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bgmzmMgaG9dJ","executionInfo":{"status":"ok","timestamp":1633734503674,"user_tz":420,"elapsed":14070,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}},"outputId":"198f63b1-8340-4818-bc22-7cc6a64defa7"},"source":["num_classes = 100\n","input_shape = (32, 32, 3)\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n","\n","print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n","print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n","\n","learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 64\n","num_epochs = 50\n","dropout_rate = 0.2\n","image_size = 64  # We'll resize input images to this size.\n","patch_size = 2  # Size of the patches to be extract from the input images.\n","num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n","latent_dim = 256  # Size of the latent array.\n","projection_dim = 256  # Embedding size of each element in the data and latent arrays.\n","num_heads = 8  # Number of Transformer heads.\n","ffn_units = [\n","    projection_dim,\n","    projection_dim,\n","]  # Size of the Transformer Feedforward network.\n","num_transformer_blocks = 4\n","num_iterations = 2  # Repetitions of the cross-attention and Transformer modules.\n","classifier_units = [\n","    projection_dim,\n","    num_classes,\n","]  # Size of the Feedforward network of the final classifier.\n","\n","print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n","print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n","print(f\"Patches per image: {num_patches}\")\n","print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n","print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n","print(f\"Data array shape: {num_patches} X {projection_dim}\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n","169009152/169001437 [==============================] - 11s 0us/step\n","169017344/169001437 [==============================] - 11s 0us/step\n","x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n","x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n","Image size: 64 X 64 = 4096\n","Patch size: 2 X 2 = 4 \n","Patches per image: 1024\n","Elements per patch (3 channels): 12\n","Latent array shape: 256 X 256\n","Data array shape: 1024 X 256\n"]}]},{"cell_type":"markdown","metadata":{"id":"QoGsxiqZHRog"},"source":["Use data augmentation"]},{"cell_type":"code","metadata":{"id":"fm4WtLPJHC3u","executionInfo":{"status":"ok","timestamp":1633734512152,"user_tz":420,"elapsed":8518,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.Normalization(),\n","        layers.Resizing(image_size, image_size),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomZoom(\n","            height_factor=0.2, width_factor=0.2\n","        ),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","# Compute the mean and the variance of the training data for normalization.\n","data_augmentation.layers[0].adapt(x_train)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-GQJe-hUHUMG"},"source":["Implement Feedforward network (FFN)"]},{"cell_type":"code","metadata":{"id":"3tS9uc-9HE33","executionInfo":{"status":"ok","timestamp":1633734512156,"user_tz":420,"elapsed":28,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["\n","def create_ffn(hidden_units, dropout_rate):\n","    ffn_layers = []\n","    for units in hidden_units[:-1]:\n","        ffn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n","\n","    ffn_layers.append(layers.Dense(units=hidden_units[-1]))\n","    ffn_layers.append(layers.Dropout(dropout_rate))\n","\n","    ffn = keras.Sequential(ffn_layers)\n","    return ffn\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RUK0wLqRHW5w"},"source":["Implement patch creation as a layer"]},{"cell_type":"code","metadata":{"id":"QJKmIlu8HWW_","executionInfo":{"status":"ok","timestamp":1633734512156,"user_tz":420,"elapsed":25,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["\n","class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjDF8cboHZWM"},"source":["Implement the patch encoding layer"]},{"cell_type":"code","metadata":{"id":"mBNNiNfcHbm7","executionInfo":{"status":"ok","timestamp":1633734512157,"user_tz":420,"elapsed":25,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["\n","class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patches):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patches) + self.position_embedding(positions)\n","        return encoded\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"efghuQwbHd0Z"},"source":["Build the Perceiver model"]},{"cell_type":"code","metadata":{"id":"8ZNH6leSHhcQ","executionInfo":{"status":"ok","timestamp":1633734512157,"user_tz":420,"elapsed":23,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["\n","def create_cross_attention_module(\n","    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate\n","):\n","\n","    inputs = {\n","        # Recieve the latent array as an input of shape [1, latent_dim, projection_dim].\n","        \"latent_array\": layers.Input(shape=(latent_dim, projection_dim)),\n","        # Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].\n","        \"data_array\": layers.Input(shape=(data_dim, projection_dim)),\n","    }\n","\n","    # Apply layer norm to the inputs\n","    latent_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"latent_array\"])\n","    data_array = layers.LayerNormalization(epsilon=1e-6)(inputs[\"data_array\"])\n","\n","    # Create query tensor: [1, latent_dim, projection_dim].\n","    query = layers.Dense(units=projection_dim)(latent_array)\n","    # Create key tensor: [batch_size, data_dim, projection_dim].\n","    key = layers.Dense(units=projection_dim)(data_array)\n","    # Create value tensor: [batch_size, data_dim, projection_dim].\n","    value = layers.Dense(units=projection_dim)(data_array)\n","\n","    # Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].\n","    attention_output = layers.Attention(use_scale=True, dropout=0.1)(\n","        [query, key, value], return_attention_scores=False\n","    )\n","    # Skip connection 1.\n","    attention_output = layers.Add()([attention_output, latent_array])\n","\n","    # Apply layer norm.\n","    attention_output = layers.LayerNormalization(epsilon=1e-6)(attention_output)\n","    # Apply Feedforward network.\n","    ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n","    outputs = ffn(attention_output)\n","    # Skip connection 2.\n","    outputs = layers.Add()([outputs, attention_output])\n","\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=outputs)\n","    return model\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BhPO7CsuHlFz"},"source":["Transformer module"]},{"cell_type":"code","metadata":{"id":"-7vvUON_Hlut","executionInfo":{"status":"ok","timestamp":1633734512157,"user_tz":420,"elapsed":22,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["\n","def create_transformer_module(\n","    latent_dim,\n","    projection_dim,\n","    num_heads,\n","    num_transformer_blocks,\n","    ffn_units,\n","    dropout_rate,\n","):\n","\n","    # input_shape: [1, latent_dim, projection_dim]\n","    inputs = layers.Input(shape=(latent_dim, projection_dim))\n","\n","    x0 = inputs\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(num_transformer_blocks):\n","        # Apply layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(x0)\n","        # Create a multi-head self-attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, x0])\n","        # Apply layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # Apply Feedforward network.\n","        ffn = create_ffn(hidden_units=ffn_units, dropout_rate=dropout_rate)\n","        x3 = ffn(x3)\n","        # Skip connection 2.\n","        x0 = layers.Add()([x3, x2])\n","\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=x0)\n","    return model\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dc_zPIXuHpKQ"},"source":["Perceiver model"]},{"cell_type":"code","metadata":{"id":"S3M9jP8MHq07","executionInfo":{"status":"ok","timestamp":1633734512158,"user_tz":420,"elapsed":21,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["\n","class Perceiver(keras.Model):\n","    def __init__(\n","        self,\n","        patch_size,\n","        data_dim,\n","        latent_dim,\n","        projection_dim,\n","        num_heads,\n","        num_transformer_blocks,\n","        ffn_units,\n","        dropout_rate,\n","        num_iterations,\n","        classifier_units,\n","    ):\n","        super(Perceiver, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        self.data_dim = data_dim\n","        self.patch_size = patch_size\n","        self.projection_dim = projection_dim\n","        self.num_heads = num_heads\n","        self.num_transformer_blocks = num_transformer_blocks\n","        self.ffn_units = ffn_units\n","        self.dropout_rate = dropout_rate\n","        self.num_iterations = num_iterations\n","        self.classifier_units = classifier_units\n","\n","    def build(self, input_shape):\n","        # Create latent array.\n","        self.latent_array = self.add_weight(\n","            shape=(self.latent_dim, self.projection_dim),\n","            initializer=\"random_normal\",\n","            trainable=True,\n","        )\n","\n","        # Create patching module.\n","        self.patcher = Patches(self.patch_size)\n","\n","        # Create patch encoder.\n","        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n","\n","        # Create cross-attenion module.\n","        self.cross_attention = create_cross_attention_module(\n","            self.latent_dim,\n","            self.data_dim,\n","            self.projection_dim,\n","            self.ffn_units,\n","            self.dropout_rate,\n","        )\n","\n","        # Create Transformer module.\n","        self.transformer = create_transformer_module(\n","            self.latent_dim,\n","            self.projection_dim,\n","            self.num_heads,\n","            self.num_transformer_blocks,\n","            self.ffn_units,\n","            self.dropout_rate,\n","        )\n","\n","        # Create global average pooling layer.\n","        self.global_average_pooling = layers.GlobalAveragePooling1D()\n","\n","        # Create a classification head.\n","        self.classification_head = create_ffn(\n","            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n","        )\n","\n","        super(Perceiver, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        # Augment data.\n","        augmented = data_augmentation(inputs)\n","        # Create patches.\n","        patches = self.patcher(augmented)\n","        # Encode patches.\n","        encoded_patches = self.patch_encoder(patches)\n","        # Prepare cross-attention inputs.\n","        cross_attention_inputs = {\n","            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n","            \"data_array\": encoded_patches,\n","        }\n","        # Apply the cross-attention and the Transformer modules iteratively.\n","        for _ in range(self.num_iterations):\n","            # Apply cross-attention from the latent array to the data array.\n","            latent_array = self.cross_attention(cross_attention_inputs)\n","            # Apply self-attention Transformer to the latent array.\n","            latent_array = self.transformer(latent_array)\n","            # Set the latent array of the next iteration.\n","            cross_attention_inputs[\"latent_array\"] = latent_array\n","\n","        # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.\n","        representation = self.global_average_pooling(latent_array)\n","        # Generate logits.\n","        logits = self.classification_head(representation)\n","        return logits\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pblzsUv4HtYo"},"source":["Compile, train, and evaluate the mode"]},{"cell_type":"code","metadata":{"id":"4hqjFKlcHvQv","executionInfo":{"status":"ok","timestamp":1633734512158,"user_tz":420,"elapsed":20,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}}},"source":["\n","def run_experiment(model):\n","\n","    # Create LAMB optimizer with weight decay.\n","    optimizer = tfa.optimizers.LAMB(\n","        learning_rate=learning_rate, weight_decay_rate=weight_decay,\n","    )\n","\n","    # Compile the model.\n","    model.compile(\n","        optimizer=optimizer,\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n","            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n","        ],\n","    )\n","\n","    # Create a learning rate scheduler callback.\n","    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n","        monitor=\"val_loss\", factor=0.2, patience=3\n","    )\n","\n","    # Create an early stopping callback.\n","    early_stopping = tf.keras.callbacks.EarlyStopping(\n","        monitor=\"val_loss\", patience=15, restore_best_weights=True\n","    )\n","\n","    # Fit the model.\n","    history = model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_split=0.1,\n","        callbacks=[early_stopping, reduce_lr],\n","    )\n","\n","    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    # Return history to plot learning curves.\n","    return history\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"B7Xd0uxHHxQG","executionInfo":{"status":"error","timestamp":1633734687038,"user_tz":420,"elapsed":36641,"user":{"displayName":"Tripura Chandana Gayatri Gorla","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04330212458877081975"}},"outputId":"d57f13ea-0cc3-4452-cd17-80b8f68b1201"},"source":["perceiver_classifier = Perceiver(\n","    patch_size,\n","    num_patches,\n","    latent_dim,\n","    projection_dim,\n","    num_heads,\n","    num_transformer_blocks,\n","    ffn_units,\n","    dropout_rate,\n","    num_iterations,\n","    classifier_units,\n",")\n","\n","\n","history = run_experiment(perceiver_classifier)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-6127dad97188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperceiver_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-203edd3b2275>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64,8,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node perceiver_1/model_1/multi_head_attention_3/einsum_3/Einsum (defined at <ipython-input-10-05efc0397541>:89) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_50204]\n\nFunction call stack:\ntrain_function\n"]}]}]}